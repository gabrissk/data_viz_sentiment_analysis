{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "from kafka import KafkaProducer, KafkaConsumer\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "reddit = praw.Reddit(client_id='yQ83iAt0TWpfgiW07sTkOw', client_secret='q9na4iAgw25R8yBgLIDiNVaMwSp7EQ', user_agent='gabrissk', check_for_async=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    def __init__(self, id, title, creation_date, subject) -> None:\n",
    "        self.id = id\n",
    "        self.title = title\n",
    "        self.creation_date = creation_date\n",
    "        self.comments = []\n",
    "        self.subject = subject\n",
    "    def add_comment(self, comment):\n",
    "        self.comments.append(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Comment:\n",
    "    def __init__(self, id, text, creation_date) -> None:\n",
    "        self.id = id\n",
    "        self.text = text\n",
    "        self.creation_date = creation_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_comment(comment, new_post):\n",
    "    # print(comment.body)\n",
    "    new_post['comments'].append(\n",
    "        {\n",
    "            \"id\": comment.id, \n",
    "            \"text\": comment.body, \n",
    "            \"creation_date\": comment.created_utc\n",
    "        }\n",
    "    )\n",
    "\n",
    "    for reply in comment.replies:\n",
    "        process_comment(reply, new_post)\n",
    "\n",
    "def process_subreddit_comments(subject, posts:list):\n",
    "    subreddit = reddit.subreddit(subject)\n",
    "    for post in subreddit.new(limit=10):\n",
    "        # print(post.title)\n",
    "        # print(post.selftext)\n",
    "        # print(post.created_utc)\n",
    "        new_post = {\n",
    "            \"id\": post.id, \n",
    "            \"title\": post.title, \n",
    "            \"creation_date\": post.created_utc,\n",
    "            \"subject\": subject,\n",
    "            \"comments\": []\n",
    "        }\n",
    "        post.comments.replace_more()\n",
    "        for comment in post.comments:\n",
    "            process_comment(comment, new_post)\n",
    "                \n",
    "        # send_post_to_kafka(new_post)\n",
    "        posts.append(new_post)\n",
    "\n",
    "\n",
    "def send_post_to_kafka(posts):\n",
    "    print(\"Sending to kafka...\")\n",
    "    producer = KafkaProducer(bootstrap_servers='localhost:9092')\n",
    "    # post_dict = vars(posts)\n",
    "    posts\n",
    "    producer.send(\"reddit-dataviz-comments\", json.dumps(posts).encode('utf-8')).get(timeout=10)\n",
    "    producer.flush()\n",
    "    producer.close\n",
    "\n",
    "subjects = [\"powerbi\", \"tableau\", \"qlikview\", \"looker\", \"datastudio\", \"domo\"]\n",
    "posts = []\n",
    "\n",
    "for subject in subjects:\n",
    "    process_subreddit_comments(subject, posts)\n",
    "    print(\"Finished reading from \", subject)\n",
    "\n",
    "send_post_to_kafka(posts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "producer = KafkaProducer(bootstrap_servers='localhost:29002')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'comment_id': 'jkwerc8', 'comment_text': 'thank you :D\\n\\nedit: I actually found the long-form video of that short to be more helpful:\\n\\n[https://www.youtube.com/watch?v=Ty3iMTDZAA0](https://www.youtube.com/watch?v=Ty3iMTDZAA0)', 'comment_creation_date': '2023-05-20 13:19:25', 'comment_subject': 'powerbi'}, {'comment_id': 'jkwdl71', 'comment_text': 'I don‚Äôt know the exact steps, but if you have access to Microsoft analysis service, maybe. You can link that with R, as it‚Äôs one of the few other places you can run DAX. \\n\\nOtherwise maybe you‚Äôll have to set all the filters in Power BI to whatever you need it to be for your R script, have the DAX measures/columns in a table, and export it as a flat file.\\n\\nAlso, can‚Äôt you recreate whatever is being done in DAX in R?', 'comment_creation_date': '2023-05-20 13:09:06', 'comment_subject': 'powerbi'}, {'comment_id': 'jkwd2pb', 'comment_text': 'I answered the question and there was no hostility just stupidity on your end.', 'comment_creation_date': '2023-05-20 13:04:34', 'comment_subject': 'powerbi'}, {'comment_id': 'jkwck89', 'comment_text': 'Why the hostility, focus on actually answering the question?', 'comment_creation_date': '2023-05-20 13:00:01', 'comment_subject': 'powerbi'}, {'comment_id': 'jkwceg3', 'comment_text': \"Right on, the button solution using bookmarks that was suggested will work well for making that.  Let me know if you have any issues and I'll be happy to help out!\", 'comment_creation_date': '2023-05-20 12:58:33', 'comment_subject': 'powerbi'}, {'comment_id': 'jkwasxv', 'comment_text': 'When the data is stored in one database and not PBI datasets. If you think data warehouse and PBI datasets are one in the same you need to go back to school!', 'comment_creation_date': '2023-05-20 12:43:56', 'comment_subject': 'powerbi'}, {'comment_id': 'jkw9uu6', 'comment_text': 'It is broad. A small/medium business could just as well gather multiple sources in Power BI datasets and call it the single source of truth. When is a data warehouse a data warehouse?', 'comment_creation_date': '2023-05-20 12:35:07', 'comment_subject': 'powerbi'}, {'comment_id': 'jkw9o3w', 'comment_text': 'This short really helped me accomplish this.\\n\\nhttps://youtube.com/shorts/ifTJ-vqWO08?feature=share', 'comment_creation_date': '2023-05-20 12:33:16', 'comment_subject': 'powerbi'}, {'comment_id': 'jkw8y4o', 'comment_text': 'Are you in UK and not US? Might need to change the locale. Everything is backwards in the UK! üòâ', 'comment_creation_date': '2023-05-20 12:26:12', 'comment_subject': 'powerbi'}, {'comment_id': 'jkw8uwj', 'comment_text': 'Use Group By on the PO column and add a column for each Note that takes the Max of that column', 'comment_creation_date': '2023-05-20 12:25:19', 'comment_subject': 'powerbi'}, {'comment_id': 'jkw889w', 'comment_text': 'No. As long as the underlying tables for the columns referenced in the field parameter are refreshed you should be fine. Field parameters reference other tables and do not contain any actual data from those tables.', 'comment_creation_date': '2023-05-20 12:18:58', 'comment_subject': 'powerbi'}, {'comment_id': 'jkw831p', 'comment_text': 'If you think doing transformation is as easy in PQ than doing so upstream in a data warehouse using SQL then more power to you but you would be in the minority on that one. \\n\\nAnd if you have to ask about what the data warehouse would like than you haven‚Äôt used on before. It‚Äôs also not a broad statement it‚Äôs a data warehouse. It‚Äôs where we gather all of our source(s) data and put in a single source of truth with transformations done.', 'comment_creation_date': '2023-05-20 12:17:29', 'comment_subject': 'powerbi'}, {'comment_id': 'jkw3zgh', 'comment_text': 'The / is often used to indicate a special character, think you might need to put it in twice if you want to see it', 'comment_creation_date': '2023-05-20 11:34:11', 'comment_subject': 'powerbi'}, {'comment_id': 'jkw3hkn', 'comment_text': \"That's really not that complex friend. It gets much more detailed than that on occasion.\", 'comment_creation_date': '2023-05-20 11:28:34', 'comment_subject': 'powerbi'}, {'comment_id': 'jkw1nhx', 'comment_text': 'Have you been usk g the portable one by any chance?', 'comment_creation_date': '2023-05-20 11:06:47', 'comment_subject': 'powerbi'}, {'comment_id': 'jkw183t', 'comment_text': 'datamarts as a pro feature, in my opinion, would be game changing', 'comment_creation_date': '2023-05-20 11:01:30', 'comment_subject': 'powerbi'}, {'comment_id': 'jkw0y3w', 'comment_text': 'Are you using dax studio to debug this if not do so.', 'comment_creation_date': '2023-05-20 10:57:58', 'comment_subject': 'powerbi'}, {'comment_id': 'jkw00sx', 'comment_text': 'Just uploaded an image of the mocked up data model if that helps to clarify. Basically the OrderHistory table records the Customer Entity while the DownloadHistory records the (parent) Customer Group. My intention was for the measure to count for each newsletter the number of subscribed customers (at Customer Group level) who had download records within the last 6 months. \\n\\nBut at this stage, I could just add a Customer Group column to the OrderHistory table if that would make things simpler!', 'comment_creation_date': '2023-05-20 10:45:56', 'comment_subject': 'powerbi'}, {'comment_id': 'jkvx9p5', 'comment_text': 'If your data source is an SQL query this is really easy to do before loading into PowerBI.', 'comment_creation_date': '2023-05-20 10:07:46', 'comment_subject': 'powerbi'}, {'comment_id': 'jkvx7lz', 'comment_text': 'I bought this . Sqlbi starts by having to read a book.. not too interactive.', 'comment_creation_date': '2023-05-20 10:06:59', 'comment_subject': 'powerbi'}, {'comment_id': 'jkvul7d', 'comment_text': 'Hello, you can use power query. There you can go to Add Column tab and select the column you need to get minutes from. After that you can use extract button, select Text between delimiters and put ( as a start delimiter and ) as an end delimiter. You will get a new column that you can renamed TW that will contains information you need.', 'comment_creation_date': '2023-05-20 09:29:21', 'comment_subject': 'powerbi'}]\n",
      "21\n",
      "[{'post_id': '13msahg', 'post_title': 'Customize label in stacked column chart', 'post_creation_date': '2023-05-20 13:37:23', 'post_subject': 'powerbi'}, {'post_id': '13mq5rc', 'post_title': \"Can I create a disconnected table with 2 columns of 2 rows each, where one column is dynamic based on TODAY()'s value?\", 'post_creation_date': '2023-05-20 12:05:38', 'post_subject': 'powerbi'}, {'post_id': '13mnluq', 'post_title': 'Direct Query Model', 'post_creation_date': '2023-05-20 09:51:14', 'post_subject': 'powerbi'}, {'post_id': '13mmn80', 'post_title': 'I need a power query code that will replace, \"N/A\" datapoints with the average (column name: Year), = Table.ReplaceValue(#\"Replaced Value2\", null, each List.Average(#\"Replaced Value2\"[North]), Replacer.ReplaceValue,{\"North\"}), I need a code similar to the one I wrote here.', 'post_creation_date': '2023-05-20 08:55:56', 'post_subject': 'powerbi'}]\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "subreddit = reddit.subreddit('powerbi')\n",
    "end_date = datetime.utcnow()  # Current time\n",
    "start_date = end_date - timedelta(hours=5)\n",
    "# Retrieve comments from the subreddit\n",
    "all_comments = subreddit.comments()\n",
    "\n",
    "# Filter comments within the specified time range\n",
    "filtered_comments = [{\n",
    "    \"comment_id\": comment.id, \n",
    "    \"comment_text\": comment.body,\n",
    "    \"comment_creation_date\": datetime.utcfromtimestamp(int(comment.created_utc)).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    \"comment_subject\": 'powerbi'\n",
    "} for comment in all_comments if start_date <= datetime.utcfromtimestamp(comment.created_utc) <= end_date]\n",
    "\n",
    "print(filtered_comments)\n",
    "print(filtered_comments.__len__())\n",
    "\n",
    "all_posts = subreddit.new(limit=100)\n",
    "filtered_posts = [{\n",
    "    \"post_id\": post.id,\n",
    "    \"post_title\": post.title,\n",
    "    \"post_creation_date\": datetime.utcfromtimestamp(int(post.created_utc)).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    \"post_subject\": 'powerbi'\n",
    "} for post in all_posts if start_date <= datetime.utcfromtimestamp(post.created_utc) <= end_date]\n",
    "\n",
    "print(filtered_posts)\n",
    "print(filtered_posts.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\dtiDIgital\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "sentiment_scores = analyzer.polarity_scores('You just create a manual table with the categories as the values. Click the Enter Data button on the ribbon and create the table. Use that column in your slicer.')\n",
    "print(sentiment_scores['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o170.json.\n: java.nio.file.AccessDeniedException: s3a://reddit-data-viz-comments/reddit-api-data: org.apache.hadoop.fs.s3a.auth.NoAuthWithAWSException: No AWS Credentials provided by TemporaryAWSCredentialsProvider SimpleAWSCredentialsProvider EnvironmentVariableCredentialsProvider IAMInstanceCredentialsProvider : com.amazonaws.SdkClientException: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY))\r\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:206)\r\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:170)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3289)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3185)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:3053)\r\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1760)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.exists(S3AFileSystem.java:4263)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:784)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:782)\r\n\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:372)\r\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\r\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\r\n\tat scala.util.Success.map(Try.scala:213)\r\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\r\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\r\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\r\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\r\n\tat java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1402)\r\n\tat java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)\r\n\tat java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)\r\n\tat java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)\r\n\tat java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157)\r\nCaused by: org.apache.hadoop.fs.s3a.auth.NoAuthWithAWSException: No AWS Credentials provided by TemporaryAWSCredentialsProvider SimpleAWSCredentialsProvider EnvironmentVariableCredentialsProvider IAMInstanceCredentialsProvider : com.amazonaws.SdkClientException: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY))\r\n\tat org.apache.hadoop.fs.s3a.AWSCredentialProviderList.getCredentials(AWSCredentialProviderList.java:216)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.getCredentialsFromContext(AmazonHttpClient.java:1257)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.runBeforeRequestHandlers(AmazonHttpClient.java:833)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:783)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)\r\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)\r\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)\r\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5212)\r\n\tat com.amazonaws.services.s3.AmazonS3Client.getBucketRegionViaHeadRequest(AmazonS3Client.java:6013)\r\n\tat com.amazonaws.services.s3.AmazonS3Client.fetchRegionFromCache(AmazonS3Client.java:5986)\r\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5196)\r\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5158)\r\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1343)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$6(S3AFileSystem.java:2066)\r\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:412)\r\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:375)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2056)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2032)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3273)\r\n\t... 19 more\r\nCaused by: com.amazonaws.SdkClientException: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY))\r\n\tat com.amazonaws.auth.EnvironmentVariableCredentialsProvider.getCredentials(EnvironmentVariableCredentialsProvider.java:50)\r\n\tat org.apache.hadoop.fs.s3a.AWSCredentialProviderList.getCredentials(AWSCredentialProviderList.java:177)\r\n\t... 40 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m sc\u001b[39m.\u001b[39m_jsc\u001b[39m.\u001b[39mhadoopConfiguration()\u001b[39m.\u001b[39mset(\u001b[39m\"\u001b[39m\u001b[39mfs.s3a.aws.credentials.provider\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39morg.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     24\u001b[0m     \u001b[39m# .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m df \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39;49mread\u001b[39m.\u001b[39;49mjson(\u001b[39m\"\u001b[39;49m\u001b[39ms3a://reddit-data-viz-comments/reddit-api-data/\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     27\u001b[0m df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mwithColumn(\u001b[39m\"\u001b[39m\u001b[39mnew_column1\u001b[39m\u001b[39m\"\u001b[39m, col(\u001b[39m\"\u001b[39m\u001b[39mid\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m     28\u001b[0m df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mwithColumn(\u001b[39m\"\u001b[39m\u001b[39mnew_column2\u001b[39m\u001b[39m\"\u001b[39m, col(\u001b[39m\"\u001b[39m\u001b[39mid\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\readwriter.py:418\u001b[0m, in \u001b[0;36mDataFrameReader.json\u001b[1;34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars, lineSep, samplingRatio, dropFieldIfAllNull, encoding, locale, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, allowNonNumericNumbers)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(path) \u001b[39m==\u001b[39m \u001b[39mlist\u001b[39m:\n\u001b[0;32m    417\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_spark\u001b[39m.\u001b[39m_sc\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 418\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_df(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jreader\u001b[39m.\u001b[39;49mjson(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_spark\u001b[39m.\u001b[39;49m_sc\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mPythonUtils\u001b[39m.\u001b[39;49mtoSeq(path)))\n\u001b[0;32m    419\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(path, RDD):\n\u001b[0;32m    421\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mfunc\u001b[39m(iterator: Iterable) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterable:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\errors\\exceptions\\captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    168\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[0;32m    170\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    171\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o170.json.\n: java.nio.file.AccessDeniedException: s3a://reddit-data-viz-comments/reddit-api-data: org.apache.hadoop.fs.s3a.auth.NoAuthWithAWSException: No AWS Credentials provided by TemporaryAWSCredentialsProvider SimpleAWSCredentialsProvider EnvironmentVariableCredentialsProvider IAMInstanceCredentialsProvider : com.amazonaws.SdkClientException: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY))\r\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:206)\r\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:170)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3289)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3185)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:3053)\r\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1760)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.exists(S3AFileSystem.java:4263)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:784)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:782)\r\n\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:372)\r\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\r\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\r\n\tat scala.util.Success.map(Try.scala:213)\r\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\r\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\r\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\r\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\r\n\tat java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1402)\r\n\tat java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)\r\n\tat java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)\r\n\tat java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)\r\n\tat java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157)\r\nCaused by: org.apache.hadoop.fs.s3a.auth.NoAuthWithAWSException: No AWS Credentials provided by TemporaryAWSCredentialsProvider SimpleAWSCredentialsProvider EnvironmentVariableCredentialsProvider IAMInstanceCredentialsProvider : com.amazonaws.SdkClientException: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY))\r\n\tat org.apache.hadoop.fs.s3a.AWSCredentialProviderList.getCredentials(AWSCredentialProviderList.java:216)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.getCredentialsFromContext(AmazonHttpClient.java:1257)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.runBeforeRequestHandlers(AmazonHttpClient.java:833)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:783)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)\r\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)\r\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)\r\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5212)\r\n\tat com.amazonaws.services.s3.AmazonS3Client.getBucketRegionViaHeadRequest(AmazonS3Client.java:6013)\r\n\tat com.amazonaws.services.s3.AmazonS3Client.fetchRegionFromCache(AmazonS3Client.java:5986)\r\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5196)\r\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5158)\r\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1343)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$6(S3AFileSystem.java:2066)\r\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:412)\r\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:375)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2056)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2032)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3273)\r\n\t... 19 more\r\nCaused by: com.amazonaws.SdkClientException: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY))\r\n\tat com.amazonaws.auth.EnvironmentVariableCredentialsProvider.getCredentials(EnvironmentVariableCredentialsProvider.java:50)\r\n\tat org.apache.hadoop.fs.s3a.AWSCredentialProviderList.getCredentials(AWSCredentialProviderList.java:177)\r\n\t... 40 more\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"S3 Data Processing\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", 'AKIAWK7TRGWZ6QIDCXEZ') \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", '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') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# remove this block if use core-site.xml and env variable\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3.awsAccessKeyId\", 'AKIAWK7TRGWZ6QIDCXEZ')\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3n.awsAccessKeyId\", 'AKIAWK7TRGWZ6QIDCXEZ')\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", 'AKIAWK7TRGWZ6QIDCXEZ')\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3.awsSecretAccessKey\", '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')\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3n.awsSecretAccessKey\", 'PQuuUM4o1fK2gHiHne1/0upIxLeBQBEVhwv2ZnhQ')\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", 'PQuuUM4o1fK2gHiHne1/0upIxLeBQBEVhwv2ZnhQ')\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3n.impl\", \"org.apache.hadoop.fs.s3native.NativeS3FileSystem\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3.impl\", \"org.apache.hadoop.fs.s3.S3FileSystem\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\")\n",
    "    # .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "\n",
    "df = spark.read.json(\"s3a://reddit-data-viz-comments/reddit-api-data/\")\n",
    "df = df.withColumn(\"new_column1\", col(\"id\"))\n",
    "df = df.withColumn(\"new_column2\", col(\"id\"))\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>subject</th>\n",
       "      <th>type</th>\n",
       "      <th>sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13mz1su</td>\n",
       "      <td>How would I implement a Today() minus 7 in cus...</td>\n",
       "      <td>2023-05-20 16:13:29</td>\n",
       "      <td>looker</td>\n",
       "      <td>post</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13mgcop</td>\n",
       "      <td>How do I create a drop-down that lets me pick ...</td>\n",
       "      <td>2023-05-20 03:14:03</td>\n",
       "      <td>looker</td>\n",
       "      <td>post</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13nc85c</td>\n",
       "      <td>TIL how to correctly work-around the lack of p...</td>\n",
       "      <td>2023-05-20 23:16:45</td>\n",
       "      <td>powerbi</td>\n",
       "      <td>post</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13n9046</td>\n",
       "      <td>Power BI is not calculating correctly and off ...</td>\n",
       "      <td>2023-05-20 21:01:03</td>\n",
       "      <td>powerbi</td>\n",
       "      <td>post</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13n7zva</td>\n",
       "      <td>üìù Power BI Use Case - Profit and Loss Statement üìù</td>\n",
       "      <td>2023-05-20 20:40:59</td>\n",
       "      <td>powerbi</td>\n",
       "      <td>post</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text   \n",
       "0  13mz1su  How would I implement a Today() minus 7 in cus...  \\\n",
       "1  13mgcop  How do I create a drop-down that lets me pick ...   \n",
       "2  13nc85c  TIL how to correctly work-around the lack of p...   \n",
       "3  13n9046  Power BI is not calculating correctly and off ...   \n",
       "4  13n7zva  üìù Power BI Use Case - Profit and Loss Statement üìù   \n",
       "\n",
       "         creation_date  subject  type  sentiment_score  \n",
       "0  2023-05-20 16:13:29   looker  post              NaN  \n",
       "1  2023-05-20 03:14:03   looker  post              NaN  \n",
       "2  2023-05-20 23:16:45  powerbi  post              NaN  \n",
       "3  2023-05-20 21:01:03  powerbi  post              NaN  \n",
       "4  2023-05-20 20:40:59  powerbi  post              NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(f'C:/Users/dtiDIgital/Downloads/part-00001-f22a3acb-f658-4e32-929f-429ade8533f5-c000.snappy.parquet')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
